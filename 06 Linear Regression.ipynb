{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([[2.3], [4.5], [9.7], [10.3]])\n",
    "y = torch.Tensor([[4.9], [9.9], [19.4], [20.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegModel, self).__init__()\n",
    "        self.linear = nn.Linear(1,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegModel()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 175.74240112304688\n",
      "epoch: 2, loss: 3.963730812072754\n",
      "epoch: 3, loss: 0.37188610434532166\n",
      "epoch: 4, loss: 0.29509326815605164\n",
      "epoch: 5, loss: 0.2917781174182892\n",
      "epoch: 6, loss: 0.290012389421463\n",
      "epoch: 7, loss: 0.28829336166381836\n",
      "epoch: 8, loss: 0.2865872383117676\n",
      "epoch: 9, loss: 0.28489571809768677\n",
      "epoch: 10, loss: 0.2832179665565491\n",
      "epoch: 11, loss: 0.2815536558628082\n",
      "epoch: 12, loss: 0.27990230917930603\n",
      "epoch: 13, loss: 0.27826443314552307\n",
      "epoch: 14, loss: 0.2766406536102295\n",
      "epoch: 15, loss: 0.2750290036201477\n",
      "epoch: 16, loss: 0.2734302282333374\n",
      "epoch: 17, loss: 0.2718445062637329\n",
      "epoch: 18, loss: 0.2702718675136566\n",
      "epoch: 19, loss: 0.26871180534362793\n",
      "epoch: 20, loss: 0.26716411113739014\n",
      "epoch: 21, loss: 0.26562902331352234\n",
      "epoch: 22, loss: 0.264105886220932\n",
      "epoch: 23, loss: 0.2625954747200012\n",
      "epoch: 24, loss: 0.26109713315963745\n",
      "epoch: 25, loss: 0.25961044430732727\n",
      "epoch: 26, loss: 0.2581360340118408\n",
      "epoch: 27, loss: 0.2566736340522766\n",
      "epoch: 28, loss: 0.2552225589752197\n",
      "epoch: 29, loss: 0.2537837326526642\n",
      "epoch: 30, loss: 0.2523563802242279\n",
      "epoch: 31, loss: 0.2509402632713318\n",
      "epoch: 32, loss: 0.24953550100326538\n",
      "epoch: 33, loss: 0.24814248085021973\n",
      "epoch: 34, loss: 0.24676083028316498\n",
      "epoch: 35, loss: 0.2453896850347519\n",
      "epoch: 36, loss: 0.24402976036071777\n",
      "epoch: 37, loss: 0.24268068373203278\n",
      "epoch: 38, loss: 0.24134260416030884\n",
      "epoch: 39, loss: 0.24001501500606537\n",
      "epoch: 40, loss: 0.23869849741458893\n",
      "epoch: 41, loss: 0.23739264905452728\n",
      "epoch: 42, loss: 0.23609678447246552\n",
      "epoch: 43, loss: 0.2348116934299469\n",
      "epoch: 44, loss: 0.23353733122348785\n",
      "epoch: 45, loss: 0.23227232694625854\n",
      "epoch: 46, loss: 0.2310178130865097\n",
      "epoch: 47, loss: 0.22977423667907715\n",
      "epoch: 48, loss: 0.22853974997997284\n",
      "epoch: 49, loss: 0.22731558978557587\n",
      "epoch: 50, loss: 0.22610078752040863\n",
      "epoch: 51, loss: 0.22489672899246216\n",
      "epoch: 52, loss: 0.22370122373104095\n",
      "epoch: 53, loss: 0.22251588106155396\n",
      "epoch: 54, loss: 0.22134001553058624\n",
      "epoch: 55, loss: 0.22017380595207214\n",
      "epoch: 56, loss: 0.21901701390743256\n",
      "epoch: 57, loss: 0.2178695648908615\n",
      "epoch: 58, loss: 0.21673054993152618\n",
      "epoch: 59, loss: 0.21560168266296387\n",
      "epoch: 60, loss: 0.21448197960853577\n",
      "epoch: 61, loss: 0.21337097883224487\n",
      "epoch: 62, loss: 0.21226826310157776\n",
      "epoch: 63, loss: 0.21117493510246277\n",
      "epoch: 64, loss: 0.21009065210819244\n",
      "epoch: 65, loss: 0.20901437103748322\n",
      "epoch: 66, loss: 0.20794779062271118\n",
      "epoch: 67, loss: 0.20688919723033905\n",
      "epoch: 68, loss: 0.20583900809288025\n",
      "epoch: 69, loss: 0.20479783415794373\n",
      "epoch: 70, loss: 0.20376424491405487\n",
      "epoch: 71, loss: 0.2027396261692047\n",
      "epoch: 72, loss: 0.201723113656044\n",
      "epoch: 73, loss: 0.20071467757225037\n",
      "epoch: 74, loss: 0.1997145414352417\n",
      "epoch: 75, loss: 0.198721781373024\n",
      "epoch: 76, loss: 0.19773772358894348\n",
      "epoch: 77, loss: 0.19676171243190765\n",
      "epoch: 78, loss: 0.1957930028438568\n",
      "epoch: 79, loss: 0.19483262300491333\n",
      "epoch: 80, loss: 0.19387926161289215\n",
      "epoch: 81, loss: 0.19293461740016937\n",
      "epoch: 82, loss: 0.1919962614774704\n",
      "epoch: 83, loss: 0.19106639921665192\n",
      "epoch: 84, loss: 0.1901441365480423\n",
      "epoch: 85, loss: 0.18922854959964752\n",
      "epoch: 86, loss: 0.18832041323184967\n",
      "epoch: 87, loss: 0.1874200850725174\n",
      "epoch: 88, loss: 0.1865273267030716\n",
      "epoch: 89, loss: 0.18564100563526154\n",
      "epoch: 90, loss: 0.18476206064224243\n",
      "epoch: 91, loss: 0.18388988077640533\n",
      "epoch: 92, loss: 0.1830248385667801\n",
      "epoch: 93, loss: 0.1821671575307846\n",
      "epoch: 94, loss: 0.18131613731384277\n",
      "epoch: 95, loss: 0.18047210574150085\n",
      "epoch: 96, loss: 0.1796347051858902\n",
      "epoch: 97, loss: 0.17880411446094513\n",
      "epoch: 98, loss: 0.17798008024692535\n",
      "epoch: 99, loss: 0.17716282606124878\n",
      "epoch: 100, loss: 0.1763518899679184\n",
      "epoch: 101, loss: 0.17554756999015808\n",
      "epoch: 102, loss: 0.17475032806396484\n",
      "epoch: 103, loss: 0.17395880818367004\n",
      "epoch: 104, loss: 0.17317402362823486\n",
      "epoch: 105, loss: 0.17239534854888916\n",
      "epoch: 106, loss: 0.1716233491897583\n",
      "epoch: 107, loss: 0.17085683345794678\n",
      "epoch: 108, loss: 0.17009709775447845\n",
      "epoch: 109, loss: 0.16934296488761902\n",
      "epoch: 110, loss: 0.16859538853168488\n",
      "epoch: 111, loss: 0.16785359382629395\n",
      "epoch: 112, loss: 0.16711769998073578\n",
      "epoch: 113, loss: 0.16638824343681335\n",
      "epoch: 114, loss: 0.165663942694664\n",
      "epoch: 115, loss: 0.1649458259344101\n",
      "epoch: 116, loss: 0.1642335057258606\n",
      "epoch: 117, loss: 0.1635267734527588\n",
      "epoch: 118, loss: 0.1628260463476181\n",
      "epoch: 119, loss: 0.16213065385818481\n",
      "epoch: 120, loss: 0.1614409238100052\n",
      "epoch: 121, loss: 0.16075704991817474\n",
      "epoch: 122, loss: 0.1600779891014099\n",
      "epoch: 123, loss: 0.15940475463867188\n",
      "epoch: 124, loss: 0.15873730182647705\n",
      "epoch: 125, loss: 0.15807460248470306\n",
      "epoch: 126, loss: 0.15741762518882751\n",
      "epoch: 127, loss: 0.1567663699388504\n",
      "epoch: 128, loss: 0.156119704246521\n",
      "epoch: 129, loss: 0.15547797083854675\n",
      "epoch: 130, loss: 0.1548418253660202\n",
      "epoch: 131, loss: 0.15421096980571747\n",
      "epoch: 132, loss: 0.15358538925647736\n",
      "epoch: 133, loss: 0.15296390652656555\n",
      "epoch: 134, loss: 0.1523483395576477\n",
      "epoch: 135, loss: 0.151737242937088\n",
      "epoch: 136, loss: 0.15113139152526855\n",
      "epoch: 137, loss: 0.15052969753742218\n",
      "epoch: 138, loss: 0.1499336212873459\n",
      "epoch: 139, loss: 0.14934185147285461\n",
      "epoch: 140, loss: 0.1487552672624588\n",
      "epoch: 141, loss: 0.14817336201667786\n",
      "epoch: 142, loss: 0.1475958377122879\n",
      "epoch: 143, loss: 0.14702315628528595\n",
      "epoch: 144, loss: 0.14645493030548096\n",
      "epoch: 145, loss: 0.14589180052280426\n",
      "epoch: 146, loss: 0.14533282816410065\n",
      "epoch: 147, loss: 0.14477787911891937\n",
      "epoch: 148, loss: 0.14422817528247833\n",
      "epoch: 149, loss: 0.14368224143981934\n",
      "epoch: 150, loss: 0.14314143359661102\n",
      "epoch: 151, loss: 0.14260441064834595\n",
      "epoch: 152, loss: 0.1420721858739853\n",
      "epoch: 153, loss: 0.141543447971344\n",
      "epoch: 154, loss: 0.1410192996263504\n",
      "epoch: 155, loss: 0.14049959182739258\n",
      "epoch: 156, loss: 0.1399846076965332\n",
      "epoch: 157, loss: 0.13947317004203796\n",
      "epoch: 158, loss: 0.1389653980731964\n",
      "epoch: 159, loss: 0.1384623944759369\n",
      "epoch: 160, loss: 0.1379629522562027\n",
      "epoch: 161, loss: 0.13746826350688934\n",
      "epoch: 162, loss: 0.13697680830955505\n",
      "epoch: 163, loss: 0.1364893615245819\n",
      "epoch: 164, loss: 0.1360064446926117\n",
      "epoch: 165, loss: 0.13552682101726532\n",
      "epoch: 166, loss: 0.13505101203918457\n",
      "epoch: 167, loss: 0.1345793604850769\n",
      "epoch: 168, loss: 0.13411185145378113\n",
      "epoch: 169, loss: 0.13364720344543457\n",
      "epoch: 170, loss: 0.1331872195005417\n",
      "epoch: 171, loss: 0.13273008167743683\n",
      "epoch: 172, loss: 0.13227730989456177\n",
      "epoch: 173, loss: 0.13182757794857025\n",
      "epoch: 174, loss: 0.13138234615325928\n",
      "epoch: 175, loss: 0.13093994557857513\n",
      "epoch: 176, loss: 0.13050146400928497\n",
      "epoch: 177, loss: 0.13006602227687836\n",
      "epoch: 178, loss: 0.12963466346263885\n",
      "epoch: 179, loss: 0.1292065680027008\n",
      "epoch: 180, loss: 0.12878206372261047\n",
      "epoch: 181, loss: 0.12836016714572906\n",
      "epoch: 182, loss: 0.12794290482997894\n",
      "epoch: 183, loss: 0.12752825021743774\n",
      "epoch: 184, loss: 0.12711673974990845\n",
      "epoch: 185, loss: 0.1267092227935791\n",
      "epoch: 186, loss: 0.12630461156368256\n",
      "epoch: 187, loss: 0.12590327858924866\n",
      "epoch: 188, loss: 0.12550534307956696\n",
      "epoch: 189, loss: 0.12510983645915985\n",
      "epoch: 190, loss: 0.12471860647201538\n",
      "epoch: 191, loss: 0.12433002144098282\n",
      "epoch: 192, loss: 0.12394467741250992\n",
      "epoch: 193, loss: 0.12356232106685638\n",
      "epoch: 194, loss: 0.12318297475576401\n",
      "epoch: 195, loss: 0.12280680984258652\n",
      "epoch: 196, loss: 0.12243324518203735\n",
      "epoch: 197, loss: 0.12206348776817322\n",
      "epoch: 198, loss: 0.12169654667377472\n",
      "epoch: 199, loss: 0.12133190780878067\n",
      "epoch: 200, loss: 0.12097092717885971\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "    pred_y = model(x)\n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "    loss = criterion(pred_y, y)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    \n",
    "    print(\"epoch: {}, loss: {}\".format(epoch+1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11.0570]], grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(torch.Tensor([[5.4]]))\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
